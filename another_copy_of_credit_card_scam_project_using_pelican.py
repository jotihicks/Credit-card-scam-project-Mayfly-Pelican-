# -*- coding: utf-8 -*-
"""Another copy of Credit card scam project Using Pelican

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eYp92yj2wfkFUTv2nxmTRjzgxHmX7LE6

## **Problem Definition**
"""

# Data Manipulation
import pandas as pd
import numpy as np
import tensorflow as tf
# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px  # For high-level visualization with Plotly
import plotly.graph_objects as go  # For low-level customization with Plotly
# Modeling
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import xgboost as xgb
import lightgbm as lgb
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

from google.colab import drive
drive.mount('/content/drive')

"""# **Loading Dataset**"""

df = pd.read_excel('/content/drive/MyDrive/creditcard.xlsx')

df2 = pd.read_csv('/content/drive/MyDrive/Credit card folder/creditcard_Test_1_2023.csv')
df3 = pd.read_csv('/content/drive/MyDrive/Credit card folder/creditcard_test_data_2.csv')
df4 = pd.read_csv('/content/drive/MyDrive/Credit card folder/Creditcard_test_data_3.csv')

df.head(10)

df2.head(10)

df3.head(10)

df.tail(10)

df.describe

print(f"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.")

df.ndim

print(df.info())

# Check for missing values
print(df.isnull().sum())

# Check for duplicate rows
print(f"Number of duplicate rows: {df.duplicated().sum()}")

# Display summary statistics
print(df.describe().transpose())

"""# **Univariate Analysis**"""

# Select numerical columns
numerical_cols = df.select_dtypes(include=np.number).columns

# Calculate the mean of each numerical column
numerical_means = df[numerical_cols].mean()

# Create a bar plot
plt.figure(figsize=(12, 8))
numerical_means.plot(kind='bar', color='green', alpha=0.8)
plt.title("Mean of Numerical Features", fontsize=10, fontweight='bold')
plt.ylabel("Mean Value", fontsize=10, fontweight='bold')
plt.xlabel("Features", fontsize=10, fontweight='bold')
plt.xticks(rotation=45, fontsize=10)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""## Categorical Features:"""

# Count plots for categorical columns
categorical_cols = df.select_dtypes(include='object').columns
for col in categorical_cols:
    plt.figure(figsize=(10, 5))
    sns.countplot(y=df[col], palette='viridis')
    plt.title(f"Count Plot of {col}")
    plt.show(10)

"""# **Bivariate Analysis**"""

sns.kdeplot(df['Amount'], cumulative=True, color='teal')
plt.title("Cumulative KDE Plot")
plt.show()

# distribution of legit transactions & fraudulent transactions
df['Class'].value_counts()

# Set up figure size
plt.figure(figsize=(15, 10))

# Plot histograms for numerical columns
df.hist(figsize=(15, 12), bins=50, edgecolor='black')
plt.suptitle("Feature Distributions Before Preprocessing", fontsize=16)
plt.show()

# Boxplots to detect outliers
plt.figure(figsize=(15, 10))
sns.boxplot(data=df, orient="h", palette="Set2")
plt.title("Boxplots of Features Before Preprocessing", fontsize=12)
plt.show()

# Display summary statistics
print("Summary Statistics:")
print(df.describe())

"""# **Data Preprocessing and feature Engineering For Pelican Algorithm**"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Assume `df` is your DataFrame and "Class" is the label column:
features = df.drop(columns=["Class"])
labels   = df["Class"]

# 1a. Scale features
scaler   = StandardScaler()
X_scaled = scaler.fit_transform(features)

# 1b. Split data
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, labels,
    test_size=0.2,
    random_state=42,
    stratify=labels
)

from sklearn.base import clone
from sklearn.metrics import roc_auc_score


class PelicanOptimizer:
    def __init__(self, estimator, param_bounds, pop_size=20, n_iter=50, seed=42):
        """
        estimator     : a scikit-learn classifier
        param_bounds  : dict, e.g. {'max_depth': (1, 20), 'min_samples_split': (2, 100)}
        pop_size      : number of pelicans in the swarm
        n_iter        : number of optimization iterations
        """
        self.estimator    = estimator
        self.param_bounds = param_bounds
        self.pop_size     = pop_size
        self.n_iter       = n_iter
        self.history      = []  # best fitness per iteration
        np.random.seed(seed)

    def _random_solution(self):
        sol = {}
        for k, (low, high) in self.param_bounds.items():
            sol[k] = np.random.uniform(low, high)
        return sol

    def _evaluate(self, sol, X, y):
        clf = clone(self.estimator)
        # round or cast integer params
        params = {k: int(round(v)) for k, v in sol.items()}
        clf.set_params(**params)
        # simple 3-fold CV AUC
        scores = []
        idx = np.arange(len(y))
        np.random.shuffle(idx)
        splits = np.array_split(idx, 3)
        for fold in range(3):
            val_idx = splits[fold]
            train_idx = np.concatenate([splits[i] for i in range(3) if i!=fold])
            clf.fit(X[train_idx], y.iloc[train_idx])
            proba = clf.predict_proba(X[val_idx])[:,1]
            scores.append(roc_auc_score(y.iloc[val_idx], proba))
        return np.mean(scores)

    def optimize(self, X, y):
        # 1.1 Initialize population
        pop = [self._random_solution() for _ in range(self.pop_size)]
        fitness = [self._evaluate(sol, X, y) for sol in pop]
        best_idx = np.argmax(fitness)
        best_sol, best_fit = pop[best_idx], fitness[best_idx]

        # 1.2 Iterations
        for t in range(self.n_iter):
            new_pop = []
            for i, sol in enumerate(pop):
                # Move towards best + random Gaussian perturbation
                direction = {k: best_sol[k] - sol[k] for k in sol}
                step = {k: sol[k] + np.random.rand() * direction[k]
                              + np.random.randn()*0.1*(self.param_bounds[k][1]-self.param_bounds[k][0])
                        for k in sol}
                # Clip to bounds
                for k,(low,high) in self.param_bounds.items():
                    step[k] = np.clip(step[k], low, high)
                new_pop.append(step)

            # 1.3 Re-evaluate
            pop = new_pop
            fitness = [self._evaluate(sol, X, y) for sol in pop]
            # Update global best
            idx = np.argmax(fitness)
            if fitness[idx] > best_fit:
                best_fit, best_sol = fitness[idx], pop[idx]
            self.history.append(best_fit)

        return best_sol, best_fit

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, r2_score
)

# 0) best_sol is the dict you found, e.g. {'max_depth':7,'min_samples_split':84}
clf_best = DecisionTreeClassifier(**best_sol, random_state=42)

# 1) Fit on full training data
clf_best.fit(X_train, y_train)

# 2) Test inputs
X_te = X_test    # scaled/encoded test features
y_te = y_test         # ground-truth labels

# 3) Get predictions and probabilities
y_pred  = clf_best.predict(X_te)
y_proba = clf_best.predict_proba(X_te)[:, 1]

# 4) Compute metrics
acc     = accuracy_score(y_te, y_pred)
prec    = precision_score(y_te, y_pred)
rec     = recall_score(y_te, y_pred)
f1      = f1_score(y_te, y_pred)
roc_auc = roc_auc_score(y_te, y_proba)
r2      = r2_score(y_te, y_proba)

# 5) Display
print("Test set results:")
print(f"  Accuracy : {acc:.4f}")
print(f"  Precision: {prec:.4f}")
print(f"  Recall   : {rec:.4f}")
print(f"  F1 Score : {f1:.4f}")
print(f"  ROC AUC  : {roc_auc:.4f}")
print(f"  R² (proba): {r2:.4f}")

# Cell 2: Confusion Matrix Visualization
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# compute confusion matrix
cm = confusion_matrix(y_te, y_pred, labels=[0, 1])

# plot
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])
fig_cm, ax_cm = plt.subplots(figsize=(5, 5))
disp.plot(ax=ax_cm, cmap="Blues", colorbar=False)
ax_cm.set_title("Confusion Matrix")
ax_cm.set_xlabel("Predicted label")
ax_cm.set_ylabel("True label")
plt.tight_layout()
plt.show()

# Cell 3: Learning Curve Visualization
from sklearn.model_selection import learning_curve

# generate learning curve data
train_sizes, train_scores, val_scores = learning_curve(
    estimator=clf_best,
    X=X_train,
    y=y_train,
    cv=5,
    scoring="roc_auc",
    train_sizes=np.linspace(0.1, 1.0, 5),
    shuffle=True,
    random_state=42,
    n_jobs=-1
)

# calculate mean and std
train_mean = np.mean(train_scores, axis=1)
train_std  = np.std(train_scores, axis=1)
val_mean   = np.mean(val_scores, axis=1)
val_std    = np.std(val_scores, axis=1)

# plot
fig_lc, ax_lc = plt.subplots(figsize=(6, 5))
ax_lc.plot(train_sizes, train_mean, marker='o', label="Training ROC AUC")
ax_lc.fill_between(train_sizes,
                   train_mean - train_std,
                   train_mean + train_std,
                   alpha=0.1)
ax_lc.plot(train_sizes, val_mean, marker='o', label="Validation ROC AUC")
ax_lc.fill_between(train_sizes,
                   val_mean - val_std,
                   val_mean + val_std,
                   alpha=0.1)
ax_lc.set_xlabel("Training set size")
ax_lc.set_ylabel("ROC AUC")
ax_lc.set_title("Learning Curve")
ax_lc.legend(loc="lower right")
plt.tight_layout()
plt.show()

# Cell X: Save your trained model
import joblib

# Save to file
joblib.dump(clf_best, 'pelican clf_best.joblib')
print("Pelican Model saved to clf_best.joblib")

# — later, in a new session or script —

# Cell Y: Load the model back
import joblib

clf_loaded = joblib.load('pelican clf_best.joblib')
print("Pelican Model loaded; ready to predict:")
print(clf_loaded)

"""# **Exploratory Data Analysis on Data Two**"""

df2.head()

df2.describe()

df2.shape

# Understanding the Data.

def column_summary(df2):
    # Return a new DataFrame with summarized information about the columns of 'df'.
    return pd.DataFrame({
        'Data Type': df2.dtypes,  # Retrieve the data type of each column (e.g., int64, object).
        'Missing Values': df2.isnull().sum(),  # Count the number of missing
        'Unique Values': df2.nunique(),  # Count the number of unique values in each column.
        'Memory Usage (bytes)': df2.memory_usage(index=False)  # Calculate memory usage for each column (excluding the index).
    })
# print the resulting DataFrame.
print(column_summary(df2))

"""There are no missing Values in Data two

# **Testing Pelican Model with The Second Dataset**
"""

import joblib
import pandas as pd
from sklearn.calibration import CalibratedClassifierCV
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    r2_score,
    confusion_matrix
)

# 1. Load the saved base model (uncalibrated)
base_model = joblib.load('/content/drive/MyDrive/Credit card folder/pelican clf_best.joblib')  # ← replace with your filepath

# 2. Split the dataset
y_2 = df2['Class']                               # ← replace 'target' with your label column
X_2 = df2.drop(columns=['Class'])

# 3. Split new data into a calibration set and a test set
#    We hold out half the data for calibration and half for evaluation
X_cal, X_test, y_cal, y_test = train_test_split(
    X_2, y_2,
    test_size=0.5,
    random_state=42,
    stratify=y_2
)

from sklearn.calibration import CalibratedClassifierCV

# 4. Calibrate the base model using sigmoid
calibrator = CalibratedClassifierCV(
    estimator=base_model,   # ← use “estimator” instead of “base_estimator”
    method='sigmoid',
    cv=5
)
calibrator.fit(X_cal, y_cal)

# 5. Generate predictions on the held-out test set
y_pred2     = calibrator.predict(X_test)
y_proba2    = calibrator.predict_proba(X_test)[:, 1]

# 6. Compute evaluation metrics using calibrated probabilities
acc     = accuracy_score(y_test, y_pred2)
prec    = precision_score(y_test, y_pred2, zero_division=0)
rec     = recall_score(y_test, y_pred2, zero_division=0)
f1      = f1_score(y_test, y_pred2, zero_division=0)
roc_auc = roc_auc_score(y_test, y_proba2)
r2      = r2_score(y_test, y_proba2)

print("Calibrated results on new data:")
print(f"  Accuracy : {acc:.4f}")
print(f"  Precision: {prec:.4f}")
print(f"  Recall   : {rec:.4f}")
print(f"  F1 Score : {f1:.4f}")
print(f"  ROC AUC  : {roc_auc:.4f}")
print(f"  R² (proba): {r2:.4f}")

cm = confusion_matrix(y_test, y_pred2)
print("\nConfusion Matrix:")
print(cm)

import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import learning_curve
from sklearn.metrics import ConfusionMatrixDisplay

# 1) Plot the confusion matrix
plt.figure(figsize=(6, 6))
ConfusionMatrixDisplay(confusion_matrix=cm).plot(cmap=plt.cm.Blues, values_format='d')
plt.title("Confusion Matrix on Test Set")
plt.tight_layout()
plt.show()

# 2) Plot a learning curve for the calibrated model using X_cal, y_cal
train_sizes, train_scores, valid_scores = learning_curve(
    estimator=calibrator,
    X=X_cal,
    y=y_cal,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    shuffle=True,
    random_state=42
)

train_mean = np.mean(train_scores, axis=1)
train_std  = np.std(train_scores,  axis=1)
valid_mean = np.mean(valid_scores, axis=1)
valid_std  = np.std(valid_scores,  axis=1)

plt.figure(figsize=(8, 6))
plt.fill_between(
    train_sizes,
    train_mean - train_std,
    train_mean + train_std,
    alpha=0.2,
    color='tab:blue',
    label='Train ± 1 std'
)
plt.fill_between(
    train_sizes,
    valid_mean - valid_std,
    valid_mean + valid_std,
    alpha=0.2,
    color='tab:orange',
    label='Validation ± 1 std'
)
plt.plot(
    train_sizes,
    train_mean,
    'o-',
    color='tab:blue',
    label='Mean Train score'
)
plt.plot(
    train_sizes,
    valid_mean,
    'o-',
    color='tab:orange',
    label='Mean Validation score'
)

plt.title("Learning Curve for Calibrated Classifier")
plt.xlabel("Number of training samples")
plt.ylabel("Accuracy")
plt.legend(loc="best")
plt.grid(True)
plt.tight_layout()
plt.show()

"""# **Exploratory Data Analysis on Data Three**"""

df3.head()

df3.describe()

df3.shape

# Understanding the Data.

def column_summary(df3):
    # Return a new DataFrame with summarized information about the columns of 'df'.
    return pd.DataFrame({
        'Data Type': df3.dtypes,  # Retrieve the data type of each column (e.g., int64, object).
        'Missing Values': df3.isnull().sum(),  # Count the number of missing
        'Unique Values': df3.nunique(),  # Count the number of unique values in each column.
        'Memory Usage (bytes)': df3.memory_usage(index=False)  # Calculate memory usage for each column (excluding the index).
    })
# print the resulting DataFrame.
print(column_summary(df3))

"""# **Testing Pelican Model With Third Dataset**"""

import joblib

# 1. Load the saved base model (uncalibrated)
base_model = joblib.load('/content/drive/MyDrive/Credit card folder/pelican clf_best.joblib')  # ← replace with your filepath

# 2. Split the dataset
y_3 = df3['Class']
X_3 = df3.drop(columns=['Class'])

X_3.columns

import pandas as pd
import joblib
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    r2_score,
    confusion_matrix
)

# Load trained model (or calibrator)
model = joblib.load('/content/drive/MyDrive/Credit card folder/pelican clf_best.joblib')

# Split df_3 into X_3 (features) and y_3 (labels)
y_3 = df3['Class']            # ← replace 'target' if your label column is named differently
X_3 = df3.drop(columns=['Class'])

# Drop any columns that were not used during training (e.g., 'id')
X_3 = X_3.drop(columns=['id'], errors='ignore')

# Because model was a CalibratedClassifierCV, it knows its training‐time feature names:
if hasattr(model, 'feature_names_in_'):
    expected_cols = list(model.feature_names_in_)
    #  Drop unexpected columns
    to_drop = [c for c in X_3.columns if c not in expected_cols]
    X_3 = X_3.drop(columns=to_drop, errors='ignore')

    # Add any missing columns (fill with zeros if truly missing)
    missing = [c for c in expected_cols if c not in X_3.columns]
    for c in missing:
        X_3[c] = 0

    # Reorder to match training order
    X_3 = X_3[expected_cols]

# Generate predictions
y_pred3  = model.predict(X_3)

# If predict_proba exists, get probabilities for AUC and R² on proba
try:
    y_proba3 = model.predict_proba(X_3)[:, 1]
    has_proba = True
except AttributeError:
    has_proba = False

# Compute metrics
acc     = accuracy_score(y_3, y_pred3)
prec    = precision_score(y_3, y_pred3, zero_division=0)
rec     = recall_score(y_3, y_pred3, zero_division=0)
f1      = f1_score(y_3, y_pred3, zero_division=0)

print("Test results on df3:")
print(f"  Accuracy : {acc:.4f}")
print(f"  Precision: {prec:.4f}")
print(f"  Recall   : {rec:.4f}")
print(f"  F1 Score : {f1:.4f}")

if has_proba:
    roc_auc = roc_auc_score(y_3, y_proba3)
    r2      = r2_score(y_3, y_proba3)
    print(f"  ROC AUC  : {roc_auc:.4f}")
    print(f"  R² (proba): {r2:.4f}")
else:
    r2 = r2_score(y_3, y_pred3)
    print(f"  R²       : {r2:.4f}")

# 9. Display confusion matrix
cm = confusion_matrix(y_3, y_pred3)
print("\nConfusion Matrix:")
print(cm)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.model_selection import learning_curve

# — ASSUMPTIONS —
# • df3 is a pandas DataFrame in the environment, containing all feature columns plus a 'target' column.
# • If df3 has columns like 'id' that were not used at training time, they should be removed.

# 1. Split df3 into X3 (features) and y3 (labels)
y3 = df3['Class']
X3 = df3.drop(columns=['Class'], errors='ignore')

# 2. Drop any columns not used during training (e.g., 'id')
X3 = X3.drop(columns=['id'], errors='ignore')

# 3. If model has feature_names_in_, align X3 to those expected features:
if hasattr(model, 'feature_names_in_'):
    expected = list(model.feature_names_in_)
    # 3a. Drop any unexpected columns
    drop_cols = [col for col in X3.columns if col not in expected]
    X3 = X3.drop(columns=drop_cols, errors='ignore')
    # 3b. Add any missing columns with zeros
    missing_cols = [col for col in expected if col not in X3.columns]
    for col in missing_cols:
        X3[col] = 0
    # 3c. Reorder to match the training order
    X3 = X3[expected]

# 4. Generate predictions on X3
y_pred3 = model.predict(X3)

# 5. Compute confusion matrix
cm = confusion_matrix(y3, y_pred3)

# 6. Plot the confusion matrix
plt.figure(figsize=(6, 6))
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap=plt.cm.Blues, values_format='d', ax=plt.gca())
plt.title("Confusion Matrix on df3")
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.tight_layout()
plt.show()

# 7. Plot the learning curve for model using df3 as both training and validation data
train_sizes, train_scores, val_scores = learning_curve(
    estimator=model,
    X=X3,
    y=y3,
    train_sizes=np.linspace(0.1, 1.0, 10),
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    shuffle=True,
    random_state=42
)

train_mean = np.mean(train_scores, axis=1)
train_std  = np.std(train_scores, axis=1)
val_mean   = np.mean(val_scores, axis=1)
val_std    = np.std(val_scores, axis=1)

plt.figure(figsize=(8, 6))
plt.fill_between(
    train_sizes,
    train_mean - train_std,
    train_mean + train_std,
    alpha=0.2,
    color='tab:blue',
    label='Train ± 1 std'
)
plt.fill_between(
    train_sizes,
    val_mean - val_std,
    val_mean + val_std,
    alpha=0.2,
    color='tab:orange',
    label='Validation ± 1 std'
)
plt.plot(train_sizes, train_mean, 'o-', color='tab:blue', label='Mean Train Score')
plt.plot(train_sizes, val_mean, 'o-', color='tab:orange', label='Mean Validation Score')

plt.title("Learning Curve on df3")
plt.xlabel("Number of training samples")
plt.ylabel("Accuracy")
plt.legend(loc="best")
plt.grid(True)
plt.tight_layout()
plt.show()

"""# **Exploratory Data Analysis on Data Four**"""

df4.head()

df4.tail()

df4.describe()

df4.info()

"""X_train_proc = preprocessor.fit_transform(X_train)    # scipy.sparse CSR
X_test_proc  = preprocessor.transform(X_test)

"""

