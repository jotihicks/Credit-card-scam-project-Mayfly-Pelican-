# -*- coding: utf-8 -*-
"""Credit card scam project Using Mayfly Original

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fwhOEg0pHG2zCRKEsvsgGmdRP-5cNH1Z

## **Problem Definition**
"""

# Data Manipulation
import pandas as pd
import numpy as np
import tensorflow as tf
# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px  # For high-level visualization with Plotly
import plotly.graph_objects as go  # For low-level customization with Plotly
# Modeling
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import xgboost as xgb
import lightgbm as lgb
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

"""# **Loading Dataset**"""

df = pd.read_excel('/content/drive/MyDrive/creditcard.xlsx')

df2 = pd.read_csv('/content/drive/MyDrive/Credit card folder/creditcard_Test_1_2023.csv')
df3 = pd.read_csv('/content/drive/MyDrive/Credit card folder/creditcard_test_data_2.csv')
df4 = pd.read_csv('/content/drive/MyDrive/Credit card folder/Creditcard_test_data_3.csv')

df.head(10)

df2.head(10)

df3.head(10)

df.tail(10)

df.describe

print(f"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.")

df.ndim

print(df.info())

# Check for missing values
print(df.isnull().sum())

# Check for duplicate rows
print(f"Number of duplicate rows: {df.duplicated().sum()}")

# Display summary statistics
print(df.describe().transpose())

"""# **Univariate Analysis**"""

# Select numerical columns
numerical_cols = df.select_dtypes(include=np.number).columns

# Calculate the mean of each numerical column
numerical_means = df[numerical_cols].mean()

# Create a bar plot
plt.figure(figsize=(12, 8))
numerical_means.plot(kind='bar', color='green', alpha=0.8)
plt.title("Mean of Numerical Features", fontsize=10, fontweight='bold')
plt.ylabel("Mean Value", fontsize=10, fontweight='bold')
plt.xlabel("Features", fontsize=10, fontweight='bold')
plt.xticks(rotation=45, fontsize=10)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""## Categorical Features:"""

# Count plots for categorical columns
categorical_cols = df.select_dtypes(include='object').columns
for col in categorical_cols:
    plt.figure(figsize=(10, 5))
    sns.countplot(y=df[col], palette='viridis')
    plt.title(f"Count Plot of {col}")
    plt.show(10)

"""# **Bivariate Analysis**"""

# Pairplot for numerical features and target variable
sns.pairplot(df, hue='Amount', diag_kind='kde')
plt.show()

# Correlation heatmap
plt.figure(figsize=(12, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Matrix")
plt.show()

sns.kdeplot(df['Amount'], cumulative=True, color='teal')
plt.title("Cumulative KDE Plot")
plt.show()

# distribution of legit transactions & fraudulent transactions
df['Class'].value_counts()

# Set up figure size
plt.figure(figsize=(15, 10))

# Plot histograms for numerical columns
df.hist(figsize=(15, 12), bins=50, edgecolor='black')
plt.suptitle("Feature Distributions Before Preprocessing", fontsize=16)
plt.show()

# Boxplots to detect outliers
plt.figure(figsize=(15, 10))
sns.boxplot(data=df, orient="h", palette="Set2")
plt.title("Boxplots of Features Before Preprocessing", fontsize=12)
plt.show()

# Display summary statistics
print("Summary Statistics:")
print(df.describe())

"""# **Data Preprocessing and feature Engineering for Mayfly Algorithm**"""

# features & target
X = df.drop('Class', axis=1)
y = df['Class']

# train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# import normally
from sklearn.model_selection import cross_val_score

# define your own alias/wrapper
def ross_val_score(estimator, X, y, cv=3, scoring='roc_auc'):
    """
    A thin wrapper around sklearn.model_selection.cross_val_score,
    here aliased as ross_val_score.
    """
    return cross_val_score(estimator, X, y, cv=cv, scoring=scoring)

# use it in fitness
def fitness(params):
    max_depth, min_samples_split = params
    clf = DecisionTreeClassifier(
        max_depth=int(max_depth),
        min_samples_split=int(min_samples_split),
        random_state=42
    )
    return ross_val_score(clf, X_train, y_train).mean()

# Mayfly Optimizer Class
from sklearn.tree            import DecisionTreeClassifier
import random


class MayflyOptimizer:
    def __init__(self, pop_size=20, n_iter=30, bounds=[(1,20),(2,100)]):
        self.pop_size = pop_size
        self.n_iter   = n_iter
        self.bounds   = bounds
        self.dim      = len(bounds)

    def _init_population(self):
        return np.array([
            [random.uniform(lb, ub) for lb, ub in self.bounds]
            for _ in range(self.pop_size)
        ])

    def optimize(self):
        males      = self._init_population()
        velocities = np.zeros_like(males)
        females    = males.copy()
        best_params, best_score = None, -np.inf

        for t in range(self.n_iter):
            # PSO‐style male update
            for i in range(self.pop_size):
                velocities[i] += random.random() * (females[i] - males[i])
                males[i]     += velocities[i]
                # enforce bounds
                for d in range(self.dim):
                    lb, ub = self.bounds[d]
                    males[i, d] = np.clip(males[i, d], lb, ub)

            # female attracted to the best male
            male_scores = [fitness(m) for m in males]
            best_male   = males[np.argmax(male_scores)]
            for i in range(self.pop_size):
                females[i] += random.random() * (best_male - females[i])

            # evaluate females
            for ind in females:
                score = fitness(ind)
                if score > best_score:
                    best_score, best_params = score, ind.copy()

        return best_params, best_score

# Imports & Parallel setup
from joblib import Parallel, delayed
import random

# Cell 2: Vectorized MayflyOptimizer
class FastMayflyOptimizer:
    def __init__(self, pop_size=25, n_iter=20, bounds=[(1,20),(2,100)], n_jobs=-1):
        self.pop_size = pop_size
        self.n_iter   = n_iter
        self.bounds   = np.array(bounds)         # shape (dim, 2)
        self.dim      = len(bounds)
        self.n_jobs   = n_jobs

    def _init_population(self):
        # sample uniformly in each dimension
        lows  = self.bounds[:, 0]
        highs = self.bounds[:, 1]
        return np.random.uniform(lows, highs, size=(self.pop_size, self.dim))

    def optimize(self):
        # init
        males      = self._init_population()
        velocities = np.zeros_like(males)
        females    = males.copy()
        best_params = None
        best_score  = -np.inf

        for _ in range(self.n_iter):
            # --- 1) Update males (vectorized) ---
            r1 = np.random.rand(self.pop_size, self.dim)
            velocities += r1 * (females - males)
            males      += velocities

            # clip to bounds
            lows  = self.bounds[:, 0]
            highs = self.bounds[:, 1]
            males = np.clip(males, lows, highs)

            # --- 2) Compute male fitness in parallel ---
            male_scores = np.array(
                Parallel(n_jobs=self.n_jobs)(
                    delayed(fitness)(m) for m in males
                )
            )
            best_idx = male_scores.argmax()
            best_male = males[best_idx]

            # --- 3) Update females towards best male ---
            r2 = np.random.rand(self.pop_size, self.dim)
            females += r2 * (best_male - females)

            # --- 4) Compute female fitness and track global best ---
            female_scores = np.array(
                Parallel(n_jobs=self.n_jobs)(
                    delayed(fitness)(f) for f in females
                )
            )
            # find any new global best
            idx = female_scores.argmax()
            if female_scores[idx] > best_score:
                best_score  = female_scores[idx]
                best_params = females[idx].copy()

        return best_params, best_score

# Commented out IPython magic to ensure Python compatibility.
# Install Optuna into your current environment
# %pip install optuna

import optuna
from sklearn.model_selection import cross_val_score

def objective(trial):
    d = trial.suggest_int('max_depth', 1, 20)
    s = trial.suggest_int('min_samples_split', 2, 100)
    clf = DecisionTreeClassifier(max_depth=d, min_samples_split=s, random_state=42)
    return cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc', n_jobs=1).mean()

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50, n_jobs=4)
print(study.best_params, study.best_value)

from sklearn.metrics         import (
    classification_report,
    confusion_matrix,
    ConfusionMatrixDisplay,
    roc_auc_score
)
from sklearn.model_selection import learning_curve

# Best hyperparameters from Optuna
best_max_depth        = 11
best_min_samples_split = 100

# Final classifier
clf_final = DecisionTreeClassifier(
    max_depth=best_max_depth,
    min_samples_split=best_min_samples_split,
    random_state=42
)

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose       import ColumnTransformer

# Suppose X_train, X_test already exist
numeric_features     = X_train.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X_train.select_dtypes(include=["object","category"]).columns.tolist()

preprocessor = ColumnTransformer([
    ("num", StandardScaler(), numeric_features),
    ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=False), categorical_features),
])

# Fit & transform
X_train_proc = preprocessor.fit_transform(X_train)
X_test_proc  = preprocessor.transform(X_test)

# Fit, evaluation report & additional metrics

# Fit & predict
clf_final.fit(X_train_proc, y_train)
y_pred  = clf_final.predict(X_test_proc)
y_proba = clf_final.predict_proba(X_test_proc)[:, 1]

# Classification report
from sklearn.metrics import classification_report
print("=== Classification Report ===\n")
print(classification_report(y_test, y_pred))

# Additional metrics
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    r2_score
)

acc     = accuracy_score(y_test, y_pred)
prec    = precision_score(y_test, y_pred)
rec     = recall_score(y_test, y_pred)
f1      = f1_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_proba)
r2      = r2_score(y_test, y_proba)

print("Test set results:")
print(f"  Accuracy : {acc:.4f}")
print(f"  Precision: {prec:.4f}")
print(f"  Recall   : {rec:.4f}")
print(f"  F1 Score : {f1:.4f}")
print(f"  ROC AUC  : {roc_auc:.4f}")
print(f"  R² (proba): {r2:.4f}")

# Learning Curve (AUC vs. training set size)
train_sizes, train_scores, val_scores = learning_curve(
    estimator=clf_final,
    X=X_train_proc,
    y=y_train,
    cv=5,
    scoring="roc_auc",
    train_sizes=np.linspace(0.1, 1.0, 5),
    n_jobs=-1,
    shuffle=True,
    random_state=42
)

# compute mean±std
train_mean = np.mean(train_scores, axis=1)
train_std  = np.std(train_scores,  axis=1)
val_mean   = np.mean(val_scores,   axis=1)
val_std    = np.std(val_scores,    axis=1)

# plot
plt.figure(figsize=(6,4))
plt.plot(train_sizes, train_mean, label="Train AUC")
plt.fill_between(train_sizes,
                 train_mean - train_std,
                 train_mean + train_std,
                 alpha=0.2)
plt.plot(train_sizes, val_mean, label="Validation AUC")
plt.fill_between(train_sizes,
                 val_mean - val_std,
                 val_mean + val_std,
                 alpha=0.2)
plt.title("Learning Curve")
plt.xlabel("Training Set Size")
plt.ylabel("ROC AUC")
plt.legend(loc="best")
plt.tight_layout()
plt.show()

# Confusion Matrix Plot
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=clf_final.classes_)

fig, ax = plt.subplots(figsize=(5,5))
disp.plot(ax=ax, cmap='Blues', colorbar=True)
ax.set_title("Confusion Matrix")
plt.tight_layout()
plt.show()

"""# **Save Model**"""

# Save model & pipeline

import joblib
from sklearn.pipeline import Pipeline

# Save separately
joblib.dump(preprocessor, 'mayfly preprocessor.joblib')
joblib.dump(clf_final,    'mayfly clf_final.joblib')

# Also Bundle into one Pipeline and save
full_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier',    clf_final)
])
joblib.dump(full_pipeline, 'mayfly model_pipeline.joblib')

# Code to Load model & pipeline

import joblib

# If you saved separately:
preprocessor = joblib.load('preprocessor.joblib')
clf_final    = joblib.load('clf_final.joblib')

# If you saved the bundled pipeline:
full_pipeline = joblib.load('model_pipeline.joblib')

# Then predict with:
# preds = full_pipeline.predict(X_new)
# proba = full_pipeline.predict_proba(X_new)[:,1]

"""# **Exploratory Data Analysis on Data Two**"""

df2.head()

df2.describe()

df2.shape

# Understanding the Data.

def column_summary(df2):
    # Return a new DataFrame with summarized information about the columns of 'df'.
    return pd.DataFrame({
        'Data Type': df2.dtypes,  # Retrieve the data type of each column (e.g., int64, object).
        'Missing Values': df2.isnull().sum(),  # Count the number of missing
        'Unique Values': df2.nunique(),  # Count the number of unique values in each column.
        'Memory Usage (bytes)': df2.memory_usage(index=False)  # Calculate memory usage for each column (excluding the index).
    })
# print the resulting DataFrame.
print(column_summary(df2))

"""There are no missing Values in Data two

# **Testing The Mayfly Model with The Second Dataset**
"""

# Cell 1: Imports and load the saved model
import joblib
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    r2_score,
)

# Replace the path below with wherever your model file resides
MODEL_PATH = "/content/drive/MyDrive/Credit card folder/mayfly clf_final.joblib"
model = joblib.load(MODEL_PATH)
print(f"Loaded model: {model}")

# Split df2 into Train and Test

TARGET_COLUMN = "Class"

# Split into X_new and y_new
X_new = df2.drop(columns=[TARGET_COLUMN])
y_new = df2[TARGET_COLUMN]

# Make predictions and compute evaluation metrics

# 1. Generate class predictions
y_pred = model.predict(X_new)

# 2. If model supports predict_proba (e.g. classification), get probabilities for ROC‐AUC
try:
    y_proba = model.predict_proba(X_new)[:, 1]
    has_proba = True
except AttributeError:
    has_proba = False

# 3. Compute common classification metrics
acc = accuracy_score(y_new, y_pred)
prec = precision_score(y_new, y_pred, zero_division=0)
rec = recall_score(y_new, y_pred, zero_division=0)
f1 = f1_score(y_new, y_pred, zero_division=0)

print("Test set results:")
print(f"  Accuracy : {acc:.4f}")
print(f"  Precision: {prec:.4f}")
print(f"  Recall   : {rec:.4f}")
print(f"  F1 Score : {f1:.4f}")

if has_proba:
    roc_auc = roc_auc_score(y_new, y_proba)
    r2 = r2_score(y_new, y_proba)  # R² on probabilities
    print(f"  ROC AUC  : {roc_auc:.4f}")
    print(f"  R²       : {r2:.4f}")
else:
    r2 = r2_score(y_new, y_pred)   # R² on predicted classes
    print(f"  R²       : {r2:.4f}")

# 4. (Optional) Display confusion matrix
cm = confusion_matrix(y_new, y_pred)
print("\nConfusion Matrix:")
print(cm)

from sklearn.model_selection import learning_curve

# Choose the range of training‐set sizes (e.g., 10%, 20%, … 100% of your data)
train_sizes, train_scores, val_scores = learning_curve(
    estimator=model,
    X=X_new,
    y=y_new,
    train_sizes=np.linspace(0.1, 1.0, 10),  # 10 steps from 10% to 100%
    cv=5,                                   # 5‐fold cross‐validation
    scoring='accuracy',                     # or another metric of interest
    n_jobs=-1                               # use all G PU cores
)

# Compute the mean and standard deviation across folds
train_mean = np.mean(train_scores, axis=1)
train_std  = np.std(train_scores, axis=1)
val_mean   = np.mean(val_scores, axis=1)
val_std    = np.std(val_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, label='Training score')
plt.fill_between(
    train_sizes,
    train_mean - train_std,
    train_mean + train_std,
    alpha=0.2
)
plt.plot(train_sizes, val_mean, label='Validation score')
plt.fill_between(
    train_sizes,
    val_mean - val_std,
    val_mean + val_std,
    alpha=0.2
)

plt.title('Learning Curve')
plt.xlabel('Number of training samples')
plt.ylabel('Accuracy')
plt.legend(loc='best')
plt.grid(True)  # optional: add grid lines
plt.tight_layout()

# Show the plot
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Plotting Confusion Matrix
cm = confusion_matrix(y_new, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                              display_labels=model.classes_)

fig, ax = plt.subplots(figsize=(6, 6))
disp.plot(ax=ax, cmap=None)   # Let matplotlib pick default colors
ax.set_title("Confusion Matrix on df2")
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.tight_layout()
plt.show()

"""## Parameter Tuning with sigmoid on second dataset"""

from sklearn.calibration import CalibratedClassifierCV
# 1) Using the 'estimator' keyword
calibrator = CalibratedClassifierCV(
    estimator=model,
    method="sigmoid",
    cv=5
)

# 2) Passing your model as the first positional argument
# calibrator = CalibratedClassifierCV(model, method="sigmoid", cv=5)

calibrator.fit(X_new, y_new)
print("Calibration (sigmoid) complete.")

# Cell 5: Generate calibrated probabilities on X_new
y_proba_calibrated = calibrator.predict_proba(X_new)[:, 1]

# Cell 6: Tune the classification threshold to maximize F1 on (X_new, y_new)
thresholds = np.linspace(0.00, 1.00, 101)
f1_scores = []
for t in thresholds:
    y_pred_thresh = (y_proba_calibrated >= t).astype(int)
    f1_scores.append(f1_score(y_new, y_pred_thresh, zero_division=0))

best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx]
best_f1 = f1_scores[best_idx]

print(f"Best threshold = {best_threshold:.2f}, F1 = {best_f1:.4f}")

# Cell 7: Make final predictions at the tuned threshold
y_pred_final = (y_proba_calibrated >= best_threshold).astype(int)

# Cell 8: Compute and print evaluation metrics using the calibrated probabilities

acc     = accuracy_score(y_new, y_pred_final)
prec    = precision_score(y_new, y_pred_final, zero_division=0)
rec     = recall_score(y_new, y_pred_final, zero_division=0)
f1      = f1_score(y_new, y_pred_final, zero_division=0)
roc_auc = roc_auc_score(y_new, y_proba_calibrated)
r2      = r2_score(y_new, y_proba_calibrated)

print("Calibrated + Threshold-tuned results:")
print(f"  Accuracy : {acc:.4f}")
print(f"  Precision: {prec:.4f}")
print(f"  Recall   : {rec:.4f}")
print(f"  F1 Score : {f1:.4f}")
print(f"  ROC AUC  : {roc_auc:.4f}")
print(f"  R² (proba): {r2:.4f}")

cm = confusion_matrix(y_new, y_pred_final)
print("\nConfusion Matrix at threshold", best_threshold)
print(cm)

from sklearn.metrics import accuracy_score, precision_score, recall_score

#  Assume:
#   • y_true = y_new         (true labels for your test set)
#   • y_probs = y_proba_calibrated  (predicted probabilities)

thresholds = np.linspace(0.0, 1.0, 101)  # 0.00, 0.01, 0.02, …, 1.00

acc_list  = []
prec_list = []
rec_list  = []

for thr in thresholds:
    y_pred_thr = (y_probs >= thr).astype(int)
    acc_list.append(accuracy_score(y_true, y_pred_thr))
    prec_list.append(precision_score(y_true, y_pred_thr, zero_division=0))
    rec_list.append(recall_score(y_true, y_pred_thr, zero_division=0))

plt.figure(figsize=(8, 6))
plt.plot(thresholds, acc_list,  label="Accuracy")
plt.plot(thresholds, prec_list, label="Precision")
plt.plot(thresholds, rec_list,  label="Recall")
plt.xlabel("Decision threshold")
plt.ylabel("Score")
plt.title("Accuracy/Precision/Recall vs Decision Threshold")
plt.legend(loc="best")
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# 1. Compute confusion matrix after Parameter tunning
cm = confusion_matrix(y_new, y_pred_final)

# 2. Print the raw matrix
print(f"\nConfusion Matrix at threshold {best_threshold}:")
print(cm)

# 3. (Optional) Display as a heatmap
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(cmap="Blues")
plt.title(f"Confusion Matrix after Parameter tunning (threshold = {best_threshold:.2f})")
plt.xlabel("Predicted label")
plt.ylabel("True label")
plt.show()

"""# **Exploratory Data Analysis on Data Three**"""

df3.head()

df3.describe()

df3.shape

# Understanding the Data.

def column_summary(df3):
    # Return a new DataFrame with summarized information about the columns of 'df'.
    return pd.DataFrame({
        'Data Type': df3.dtypes,  # Retrieve the data type of each column (e.g., int64, object).
        'Missing Values': df3.isnull().sum(),  # Count the number of missing
        'Unique Values': df3.nunique(),  # Count the number of unique values in each column.
        'Memory Usage (bytes)': df3.memory_usage(index=False)  # Calculate memory usage for each column (excluding the index).
    })
# print the resulting DataFrame.
print(column_summary(df3))

"""# **Testing Mayfly with Third Dataset**"""

# Split df3 into Train and Test

TARGET_COLUMN = "Class"

# Split into X_new and y_new
X_3 = df3.drop(columns=[TARGET_COLUMN])
y_3 = df3[TARGET_COLUMN]

# Make predictions and compute evaluation metrics

# 1. Generate class predictions
y_pred3 = model.predict(X_3)

# 2. If model supports predict_proba (e.g. classification), get probabilities for ROC‐AUC
try:
    y_proba3 = model.predict_proba(X_3)[:, 1]
    has_proba = True
except AttributeError:
    has_proba = False

# 3. Compute common classification metrics
acc = accuracy_score(y_3, y_pred3)
prec = precision_score(y_3, y_pred3, zero_division=0)
rec = recall_score(y_3, y_pred3, zero_division=0)
f1 = f1_score(y_3, y_pred3, zero_division=0)

print("Test set results:")
print(f"  Accuracy : {acc:.4f}")# Cell 8: Make predictions on X_3 and compute evaluation metrics against y_3


# 1. Generate class predictions on X_3
y_pred3 = model.predict(X_3)

# 2. If model supports predict_proba, get probabilities for ROC‐AUC
try:
    y_proba3 = model.predict_proba(X_3)[:, 1]
    has_proba = True
except AttributeError:
    has_proba = False

# 3. Compute common classification metrics (all measured against y_3)
acc  = accuracy_score(y_3, y_pred3)
prec = precision_score(y_3, y_pred3, zero_division=0)
rec  = recall_score(y_3, y_pred3, zero_division=0)
f1   = f1_score(y_3, y_pred3, zero_division=0)

print("Test set results:")
print(f"  Accuracy : {acc:.4f}")
print(f"  Precision: {prec:.4f}")
print(f"  Recall   : {rec:.4f}")
print(f"  F1 Score : {f1:.4f}")

if has_proba:
    # Use y_3 (not y_new!) since y_proba comes from X_3
    roc_auc = roc_auc_score(y_3, y_proba3)
    r2      = r2_score(y_3, y_proba3)  # R² on probabilities
    print(f"  ROC AUC  : {roc_auc:.4f}")
    print(f"  R²       : {r2:.4f}")
else:
    # If no probabilities are available, fall back to R² on the binary predictions
    r2 = r2_score(y_3, y_pred3)
    print(f"  R²       : {r2:.4f}")

# 4. Display confusion matrix (also against y_3)
cm = confusion_matrix(y_3, y_pred3)
print("\nConfusion Matrix:")
print(cm)

print(f"  Precision: {prec:.4f}")
print(f"  Recall   : {rec:.4f}")
print(f"  F1 Score : {f1:.4f}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# 1. Compute confusion matrix using y_3 and y_pred
cm = confusion_matrix(y_3, y_pred3)

# 2. Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
fig, ax = plt.subplots(figsize=(6, 6))
disp.plot(ax=ax, cmap="Blues", colorbar=True)

ax.set_title("Confusion Matrix on df3")
ax.set_xlabel("Predicted Label")
ax.set_ylabel("True Label")

plt.show()

import joblib

# 1.1 Load the saved CalibratedClassifierCV
calibrator = joblib.load('mayfly_calibrator_sigmoid.joblib')

# 1.2 See exactly what column names (and order) the model was trained on:
expected_features = list(calibrator.feature_names_in_)
print("Model expects these features (in this order):")
print(expected_features)

# Separate features and target
y_3 = df3['Class']
X_3 = df3.drop(columns=['Class'])

unexpected = [col for col in X_new.columns if col not in expected_features]
missing    = [col for col in expected_features if col not in X_new.columns]

"""# **Exploratory Data Analysis on Data Four**"""

df4.head()

df4.tail()

df4.describe()

df4.info()

# Understanding the Data.

def column_summary(df4):
    # Return a new DataFrame with summarized information about the columns of 'df'.
    return pd.DataFrame({
        'Data Type': df4.dtypes,  # Retrieve the data type of each column (e.g., int64, object).
        'Missing Values': df4.isnull().sum(),  # Count the number of missing
        'Unique Values': df4.nunique(),  # Count the number of unique values in each column.
        'Memory Usage (bytes)': df4.memory_usage(index=False)  # Calculate memory usage for each column (excluding the index).
    })
# print the resulting DataFrame.
print(column_summary(df4))

"""# **Testing Mayfly With Fourth Dataset**"""

import pandas as pd
from sklearn.model_selection import train_test_split



# 2. Define the dependent (y_4) and independent (X_4) variables
y_4 = df4['isFraud']
X_4 = df4.drop(columns=['isFraud'])

# 3. Drop high-cardinality ID columns that are not directly useful as numeric features
X_4 = X_4.drop(columns=['nameOrig', 'nameDest'])

# 4. One-hot encode any remaining categorical (object) columns
#    In this dataset, "type" is object-typed (e.g. 'PAYMENT', 'TRANSFER', etc.)
X_4 = pd.get_dummies(X_4, columns=['type'], drop_first=True)

# 5. Now X_4 contains only numeric columns:
#    ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest',
#     'type_PAYMENT', 'type_TRANSFER', ...]  (depending on how many types exist)

# 6. Split into train and test sets
#    Stratify on y_4 to preserve the same fraud/non-fraud ratio in both splits
X_4_train, X_4_test, y_4_train, y_4_test = train_test_split(
    X_4,
    y_4,
    test_size=0.20,
    random_state=42,
    stratify=y_4
)

# 7. (Optional) Verify shapes
print("X_4_train shape:", X_4_train.shape)
print("y_4_train shape:", y_4_train.shape)
print("X_4_test  shape:", X_4_test.shape)
print("y_4_test  shape:", y_4_test.shape)

# 2.1 Print each column’s dtype
print(X_4.dtypes)

# 2.2 Identify only the object (string)/category columns:
non_numeric_cols = X_4.select_dtypes(include=['object', 'category']).columns.tolist()
print("Non-numeric (string) columns in X_4:", non_numeric_cols)

"""for the last Dataset that was choosen we cant pass 11 inputs in because our model expects 30 inputs"""

