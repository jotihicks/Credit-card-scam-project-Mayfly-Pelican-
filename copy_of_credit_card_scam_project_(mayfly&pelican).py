# -*- coding: utf-8 -*-
"""Copy of Credit card scam project (Mayfly&Pelican)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13vr-WtYhoviaoC-CHp4V6SmU2mCRciV6

## **Problem Definition**
"""

# Data Manipulation
import pandas as pd
import numpy as np
import tensorflow as tf
# Visualization
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px  # For high-level visualization with Plotly
import plotly.graph_objects as go  # For low-level customization with Plotly
# Modeling
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import xgboost as xgb
import lightgbm as lgb
import torch
import torch.nn as nn
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report

"""# **Loading Dataset**"""

df = pd.read_excel('/content/drive/MyDrive/creditcard.xlsx')

df2 = pd.read_csv('/content/drive/MyDrive/Credit card folder/creditcard_Test_1_2023.csv')
df3 = pd.read_csv('/content/drive/MyDrive/Credit card folder/creditcard_test_data_2.csv')
df4 = pd.read_csv('/content/drive/MyDrive/Credit card folder/Creditcard_test_data_3.csv')

df.head(10)

df2.head(10)

df3.head(10)

df.tail(10)

df.describe

print(f"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.")

df.ndim

print(df.info())

# Check for missing values
print(df.isnull().sum())

# Check for duplicate rows
print(f"Number of duplicate rows: {df.duplicated().sum()}")

# Display summary statistics
print(df.describe().transpose())

"""# **Univariate Analysis**"""

# Select numerical columns
numerical_cols = df.select_dtypes(include=np.number).columns

# Calculate the mean of each numerical column
numerical_means = df[numerical_cols].mean()

# Create a bar plot
plt.figure(figsize=(12, 8))
numerical_means.plot(kind='bar', color='green', alpha=0.8)
plt.title("Mean of Numerical Features", fontsize=10, fontweight='bold')
plt.ylabel("Mean Value", fontsize=10, fontweight='bold')
plt.xlabel("Features", fontsize=10, fontweight='bold')
plt.xticks(rotation=45, fontsize=10)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

"""## Categorical Features:"""

# Count plots for categorical columns
categorical_cols = df.select_dtypes(include='object').columns
for col in categorical_cols:
    plt.figure(figsize=(10, 5))
    sns.countplot(y=df[col], palette='viridis')
    plt.title(f"Count Plot of {col}")
    plt.show(10)

"""# **Bivariate Analysis**"""

# Pairplot for numerical features and target variable
sns.pairplot(df, hue='Amount', diag_kind='kde')
plt.show()

# Correlation heatmap
plt.figure(figsize=(12, 8))
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap="coolwarm")
plt.title("Correlation Matrix")
plt.show()

sns.kdeplot(df['Amount'], cumulative=True, color='teal')
plt.title("Cumulative KDE Plot")
plt.show()

# distribution of legit transactions & fraudulent transactions
df['Class'].value_counts()

# Set up figure size
plt.figure(figsize=(15, 10))

# Plot histograms for numerical columns
df.hist(figsize=(15, 12), bins=50, edgecolor='black')
plt.suptitle("Feature Distributions Before Preprocessing", fontsize=16)
plt.show()

# Boxplots to detect outliers
plt.figure(figsize=(15, 10))
sns.boxplot(data=df, orient="h", palette="Set2")
plt.title("Boxplots of Features Before Preprocessing", fontsize=12)
plt.show()

# Display summary statistics
print("Summary Statistics:")
print(df.describe())

"""# **Data Preprocessing and feature Engineering**"""

features = df.drop(columns=["Class"])
labels = df["Class"]
scaler = StandardScaler()
features = scaler.fit_transform(features)
X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)

X = df.drop("Class", axis=1)
y = df["Class"]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Scale and Encode

from sklearn.compose      import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder

num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = X.select_dtypes(include=["object", "category"]).columns.tolist()

preprocessor = ColumnTransformer([
    ("scale",  StandardScaler(),                   num_cols),
    ("encode", OneHotEncoder(handle_unknown="ignore",
                             sparse_output=False), cat_cols),
])

X_train_proc = preprocessor.fit_transform(X_train)
X_test_proc  = preprocessor.transform(X_test)

# Implementation Scaffold

from sklearn.base       import BaseEstimator, ClassifierMixin
from sklearn.tree       import DecisionTreeClassifier
from sklearn.metrics    import f1_score
from tqdm.auto          import tqdm

class PelicanMayflyHybridDetector(BaseEstimator, ClassifierMixin):
    """
    Hybrid optimisation to search for model hyperparameters
    or feature-subset masks that maximise F1 on fraud detection.
    """
    def __init__(
        self,
        n_particles=30,
        max_iter=50,
        w_may=0.9,    # inertia for mayfly
        w_pel=0.4,    # inertia for pelican
        c1=1.2,       # cognitive coeff
        c2=1.2,       # social coeff
        decay=0.97,
        random_state=42
    ):
        self.n_particles = n_particles
        self.max_iter     = max_iter
        self.w_may        = w_may
        self.w_pel        = w_pel
        self.c1           = c1
        self.c2           = c2
        self.decay        = decay
        self.random_state = random_state

    def _init_swarm(self, dim):
        rng = np.random.RandomState(self.random_state)
        # positions in [0,1], represent continuous relaxations of e.g. thresholds or mask probabilities
        self.X = rng.rand(self.n_particles, dim)
        self.V = rng.rand(self.n_particles, dim) * 0.1
        self.pbest = self.X.copy()
        self.pf_scores = np.zeros(self.n_particles)
        # evaluate initial personal bests
        for i in range(self.n_particles):
            self.pf_scores[i] = self._fitness(self.X[i])
        idx = np.argmax(self.pf_scores)
        self.gbest = self.X[idx].copy()
        self.gbest_score = self.pf_scores[idx]

    def _fitness(self, particle):
        # Example: interpret particle vector as feature-mask, train quick tree
        mask = particle > 0.5
        if not mask.any():
            return 0.0
        Xm = self.X_train[:, mask]
        clf = DecisionTreeClassifier(max_depth=3, random_state=self.random_state)
        clf.fit(Xm, self.y_train)
        preds = clf.predict(Xm)
        return f1_score(self.y_train, preds)

    def fit(self, X, y):
        # store data for fitness calls
        self.X_train = X
        self.y_train = y
        dim = X.shape[1]
        self._init_swarm(dim)

        for t in tqdm(range(self.max_iter)):
            for i in range(self.n_particles):
                # Mayfly update (exploration)
                r1, r2 = np.random.rand(), np.random.rand()
                cognitive = self.c1 * r1 * (self.pbest[i] - self.X[i])
                social    = self.c2 * r2 * (self.gbest  - self.X[i])
                self.V[i] = self.w_may * self.V[i] + cognitive + social

                # Pelican update (local exploitation)
                delta = np.random.randn(dim) * (self.gbest - self.X[i])
                self.V[i] = self.w_pel * self.V[i] + delta

                # Move
                self.X[i] = self.X[i] + self.V[i]
                self.X[i] = np.clip(self.X[i], 0, 1)

                # Evaluate
                score = self._fitness(self.X[i])
                if score > self.pf_scores[i]:
                    self.pbest[i]    = self.X[i].copy()
                    self.pf_scores[i] = score

                if score > self.gbest_score:
                    self.gbest       = self.X[i].copy()
                    self.gbest_score = score

            # optional decay of inertia
            self.w_may *= self.decay
            self.w_pel *= self.decay

        return self

    def predict(self, X):
        # Use the global best mask to train a final model
        mask = self.gbest > 0.5
        self.clf_final = DecisionTreeClassifier(max_depth=3, random_state=self.random_state)
        self.clf_final.fit(self.X_train[:, mask], self.y_train)
        return self.clf_final.predict(X[:, mask])

from numba import njit
from joblib import Parallel, delayed
import numpy as np

@njit
def vector_update(x, v, pbest, gbest, w, c1, c2):
    n,d = x.shape
    for i in range(n):
        for j in range(d):
            r1 = np.random.rand()
            r2 = np.random.rand()
            v[i,j] = w*v[i,j] + c1*r1*(pbest[i,j]-x[i,j]) + c2*r2*(gbest[j]-x[i,j])
            x[i,j] += v[i,j]
    return x, v

class PelicanMayflyHybridDetector:
    def fit(self, X, y):
        n,d = X.shape
        x = np.random.rand(self.n_particles, d)
        v = np.zeros_like(x)
        pbest = x.copy()
        pbest_f = np.full(self.n_particles, np.inf)
        gbest = np.zeros(d)
        gbest_f = np.inf

        for t in range(self.max_iter):
            # parallel fitness
            fs = Parallel(n_jobs=-1)(
                delayed(self._fitness)(x[i], y) for i in range(self.n_particles)
            )
            fs = np.array(fs)

            # update personal & global bests
            better = fs < pbest_f
            pbest[better] = x[better]
            pbest_f[better] = fs[better]
            if pbest_f.min() < gbest_f:
                gbest_f = pbest_f.min()
                gbest = pbest[pbest_f.argmin()]
                no_improve = 0
            else:
                no_improve += 1
                if no_improve >= 10:
                    break

            # vectorized update
            x, v = vector_update(x, v, pbest, gbest,
                                 self.w_may, self.w_pel,
                                 self.c1, self.c2)

        self.gbest_, self.gbest_f_ = gbest, gbest_f
        return self

# Model Evaluation and Training

# assume X_train_proc, X_test_proc, y_train, y_test from earlier
hybrid = PelicanMayflyHybridDetector(
    n_particles=50,
    max_iter=100,
    w_may=0.8,
    w_pel=0.5,
    c1=1.5,
    c2=1.5,
    decay=0.95
)
hybrid.fit(X_train_proc, y_train)
preds = hybrid.predict(X_test_proc)

from sklearn.metrics import classification_report
print(classification_report(y_test, preds))

"""# **Exploratory Data Analysis on Data Two**"""

df2.head()

df2.describe()

df2.shape

# Understanding the Data.

def column_summary(df2):
    # Return a new DataFrame with summarized information about the columns of 'df'.
    return pd.DataFrame({
        'Data Type': df2.dtypes,  # Retrieve the data type of each column (e.g., int64, object).
        'Missing Values': df2.isnull().sum(),  # Count the number of missing
        'Unique Values': df2.nunique(),  # Count the number of unique values in each column.
        'Memory Usage (bytes)': df2.memory_usage(index=False)  # Calculate memory usage for each column (excluding the index).
    })
# print the resulting DataFrame.
print(column_summary(df2))

"""There are no missing Values in Data two

# **Exploratory Data Analysis on Data Three**
"""

df3.head()

df3.describe()

df3.shape

# Understanding the Data.

def column_summary(df3):
    # Return a new DataFrame with summarized information about the columns of 'df'.
    return pd.DataFrame({
        'Data Type': df3.dtypes,  # Retrieve the data type of each column (e.g., int64, object).
        'Missing Values': df3.isnull().sum(),  # Count the number of missing
        'Unique Values': df3.nunique(),  # Count the number of unique values in each column.
        'Memory Usage (bytes)': df3.memory_usage(index=False)  # Calculate memory usage for each column (excluding the index).
    })
# print the resulting DataFrame.
print(column_summary(df3))

"""# **Exploratory Data Analysis on Data Four**"""

df4.head()

df4.tail()

df4.describe()

df4.info()

# Understanding the Data.

def column_summary(df4):
    # Return a new DataFrame with summarized information about the columns of 'df'.
    return pd.DataFrame({
        'Data Type': df4.dtypes,  # Retrieve the data type of each column (e.g., int64, object).
        'Missing Values': df4.isnull().sum(),  # Count the number of missing
        'Unique Values': df4.nunique(),  # Count the number of unique values in each column.
        'Memory Usage (bytes)': df4.memory_usage(index=False)  # Calculate memory usage for each column (excluding the index).
    })
# print the resulting DataFrame.
print(column_summary(df4))

X = df.drop("isFraud", axis=1)
y = df["isFraud"]
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

from sklearn.compose        import ColumnTransformer
from sklearn.preprocessing  import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from scipy import sparse

# 1) Define X/y
X = df4.drop("isFraud", axis=1)
y = df4["isFraud"]

# 2) Identify column types
numeric_features     = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(include=["object", "category"]).columns.tolist()

# 3) Sparse pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_features),
        ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=True), categorical_features),
    ],
    sparse_threshold=0.0  # force the entire output to be sparse
)

# 4) Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

# 5) Fit & transform → still sparse
X_train_proc = preprocessor.fit_transform(X_train)  # scipy.sparse matrix
X_test_proc  = preprocessor.transform(X_test)

# 6) (Optional) Convert to CSR for efficiency
X_train_proc = sparse.csr_matrix(X_train_proc)
X_test_proc  = sparse.csr_matrix(X_test_proc)

preprocessor = ColumnTransformer(
    transformers=[
      ("num", StandardScaler(), numeric_features),
      ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=True), categorical_features),
    ],
    remainder="drop"          # (optional) explicitly drop any other columns
)

X_train_proc = preprocessor.fit_transform(X_train)    # scipy.sparse CSR
X_test_proc  = preprocessor.transform(X_test)

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_fscore_support, accuracy_score

# 1. Split
X = df4.drop("isFraud", axis=1)
y = df4["isFraud"]

# 2. Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, shuffle=True
)

# 3. Identify columns
numeric_features     = X.select_dtypes(include=[np.number]).columns.tolist()
categorical_features = X.select_dtypes(include=["object", "category"]).columns.tolist()

# 4. Sparse preprocessor
preprocessor = ColumnTransformer(
    transformers=[
      ("num", StandardScaler(), numeric_features),
      # Return a sparse matrix instead of dense
      ("cat", OneHotEncoder(handle_unknown="ignore", sparse_output=True), categorical_features),
    ]
)

# 5. Transform (result is scipy.sparse)
X_train_proc = preprocessor.fit_transform(X_train)
X_test_proc  = preprocessor.transform(X_test)

# 6. Fit & evaluate
detector = PelicanMayflyHybridDetector(n_particles=30, max_iterations=50, random_state=42)
detector.fit(X_train_proc)          # if it accepts sparse

y_pred = detector.predict(X_test_proc)
precision, recall, f1, _ = precision_recall_fscore_support(
    y_test, y_pred, average="binary"
)
acc = accuracy_score(y_test, y_pred)

print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

